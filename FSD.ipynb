{"cells":[{"cell_type":"markdown","metadata":{"id":"47kV9o1Ni8GH"},"source":["# **Colab From https://github.com/TheLastBen/fast-stable-diffusion, if you face any issues, feel free to discuss them.** [Support](https://ko-fi.com/thelastben)\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"Y9EBc437WDOs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674047276116,"user_tz":-180,"elapsed":20151,"user":{"displayName":"Neo Build","userId":"01648788892181221637"}},"outputId":"3d762e96-99e9-48be-f9f3-fcfd87b52cad"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"CFWtw-6EPrKi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674047325004,"user_tz":-180,"elapsed":48892,"user":{"displayName":"Neo Build","userId":"01648788892181221637"}},"outputId":"0ae8e817-bc3b-4c8e-8462-f8a1e7d11a70"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;32mDONE !\n"]}],"source":["#@markdown # Install AUTOMATIC1111 repo\n","from IPython.utils import capture\n","from IPython.display import clear_output\n","from subprocess import getoutput\n","from google.colab import runtime\n","from IPython.display import display_markdown\n","import time\n","\n","\n","with capture.capture_output() as cap:\n","  fgitclone = \"git clone --depth 1 --no-checkout\" \n","  %cd /content/gdrive/MyDrive/\n","  !$fgitclone https://github.com/DHawk34/FSD-25.12.22 sd\n","  %cd sd\n","  !git sparse-checkout set stablediffusion stable-diffusion-webui\n","\n","  %cd /content/gdrive/MyDrive/sd/stable-diffusion-webui/\n","  !mkdir -p cache/{huggingface,torch}\n","  %cd /content/\n","  !ln -s /content/gdrive/MyDrive/sd/stable-diffusion-webui/cache/huggingface ../root/.cache/\n","  !ln -s /content/gdrive/MyDrive/sd/stable-diffusion-webui/cache/torch ../root/.cache/\n","  !wget -O /content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/shared.py https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/modules/shared.py\n","\n","Update_repo = True\n","if Update_repo:\n","  with capture.capture_output() as cap:\n","    !rm /content/gdrive/MyDrive/sd/stable-diffusion-webui/webui.sh  \n","    !rm /content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/paths.py\n","    !rm /content/gdrive/MyDrive/sd/stable-diffusion-webui/webui.py \n","    !rm /content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/ui.py\n","    !rm /content/gdrive/MyDrive/sd/stable-diffusion-webui/style.css\n","    %cd /content/gdrive/MyDrive/sd/stable-diffusion-webui/\n","  print('\u001b[1;32m')\n","  !git pull\n","  clear_output()\n","  print('\u001b[1;32mDONE !')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"ZGV_5H4xrOSp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674047395746,"user_tz":-180,"elapsed":70748,"user":{"displayName":"Neo Build","userId":"01648788892181221637"}},"outputId":"5b058381-0d46-4d6f-de93-58901e0d9dd7"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;32mDone, proceed\n"]}],"source":["#@markdown # Requirements\n","\n","import os\n","import time\n","from IPython.utils import capture\n","from IPython.display import clear_output\n","from re import search\n","\n","print('\u001b[1;32mInstalling requirements...')\n","with capture.capture_output() as cap:\n","  fgitclone = \"git clone --depth 1\" \n","  if not os.path.exists('/content/gdrive/MyDrive/sd/stablediffusion/src/k-diffusion/k_diffusion'):\n","    !mkdir /content/gdrive/MyDrive/sd/stablediffusion/src\n","    %cd /content/gdrive/MyDrive/sd/stablediffusion/src\n","    !$fgitclone https://github.com/CompVis/taming-transformers\n","    !$fgitclone https://github.com/openai/CLIP\n","    !$fgitclone https://github.com/salesforce/BLIP\n","    !$fgitclone https://github.com/sczhou/CodeFormer\n","    !$fgitclone https://github.com/crowsonkb/k-diffusion\n","    !mv /content/gdrive/MyDrive/sd/stablediffusion/src/CLIP /content/gdrive/MyDrive/sd/stablediffusion/src/clip\n","    !mv  /content/gdrive/MyDrive/sd/stablediffusion/src/BLIP /content/gdrive/MyDrive/sd/stablediffusion/src/blip\n","    !mv  /content/gdrive/MyDrive/sd/stablediffusion/src/CodeFormer /content/gdrive/MyDrive/sd/stablediffusion/src/codeformer\n","    !cp -r /content/gdrive/MyDrive/sd/stablediffusion/src/k-diffusion/k_diffusion /content/gdrive/MyDrive/sd/stable-diffusion-webui/\n","\n","\n","  %cd /content/\n","  for i in range(1,6):\n","      !wget -q \"https://github.com/DHawk34/FSD-25.12.22/raw/main/Dependencies/Dependencies.{i}\"\n","      !mv \"Dependencies.{i}\" \"Dependencies.7z.00{i}\"\n","  !7z x -y Dependencies.7z.001\n","  time.sleep(2)\n","  !cp -r /content/usr/local/lib/python3.8/dist-packages /usr/local/lib/python3.8/\n","  !rm -r /content/usr\n","  for i in range(1,6):\n","      !rm \"Dependencies.7z.00{i}\"\n","  %cd /content/gdrive/MyDrive/sd/stable-diffusion-webui/modules\n","  !wget -O paths.py https://raw.githubusercontent.com/DHawk34/FSD-25.12.22/main/AUTOMATIC1111_files/paths.py\n","\n","  if os.path.exists('/content/gdrive/MyDrive/sd/stable-diffusion-webui/extensions/deforum-for-automatic1111-webui/scripts/deforum_helpers/src/infer.py'):\n","    %cd /content/gdrive/MyDrive/sd/stable-diffusion-webui/extensions/deforum-for-automatic1111-webui/scripts/deforum_helpers\n","    !wget -O animation.py https://raw.githubusercontent.com/DHawk34/FSD-25.12.22/main/AUTOMATIC1111_files/deforum/animation.py\n","    !wget -O depth.py https://raw.githubusercontent.com/DHawk34/FSD-25.12.22/main/AUTOMATIC1111_files/deforum/depth.py\n","    %cd /content/gdrive/MyDrive/sd/stable-diffusion-webui/extensions/deforum-for-automatic1111-webui/scripts/deforum_helpers/src\n","    !wget -O infer.py https://raw.githubusercontent.com/DHawk34/FSD-25.12.22/main/AUTOMATIC1111_files/deforum/infer.py\n","\n","  if not os.path.exists('/content/gdrive/MyDrive/sd/stablediffusion/repositories/BoostingMonocularDepth'):\n","    %cd /content/gdrive/MyDrive/sd/stablediffusion\n","    !mkdir repositories\n","    %cd repositories\n","    !git clone https://github.com/isl-org/MiDaS.git midas\n","    !git clone https://github.com/compphoto/BoostingMonocularDepth.git\n","  %cd /content\n","\n","base = '/content/gdrive/MyDrive/sd/stable-diffusion'\n","dirs = [base, f'{base}/src/taming-transformers', f'{base}/src/clip',\n","        f'{base}/src/GFPGAN', f'{base}/src/blip', f'{base}/src/codeformer',\n","        f'{base}/src/realesrgan', f'{base}/src/k-diffusion', f'{base}/src/ldm']\n","for d in dirs:\n","    !rm -rf {d + '/.git'}\n","clear_output()\n","print('\u001b[1;32mDone, proceed')"]},{"cell_type":"code","execution_count":4,"metadata":{"cellView":"form","id":"p4wj_txjP3TC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674047396836,"user_tz":-180,"elapsed":1100,"user":{"displayName":"Neo Build","userId":"01648788892181221637"}},"outputId":"d54fabfe-3711-4e38-8eb4-1ebed8f72ddb"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;32mUsing the trained model.\n"]}],"source":["import os\n","from IPython.display import clear_output\n","import time\n","\n","#@markdown # Model Download/Load\n","Model_Version = \"1.5\" #@param [ \"1.5\", \"V2.1-512px\", \"V2.1-768px\"]\n","\n","RunwayML_Inpainting_Model = False #@param {type:\"boolean\"}\n","\n","token = \"\" #@param {type:\"string\"}\n","\n","Redownload_the_original_model = False #@param {type:\"boolean\"}\n","\n","if Redownload_the_original_model:\n","  if os.path.exists('/content/mainmodel.ckpt'):\n","    !rm /content/mainmodel.ckpt\n","  %cd /content/gdrive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion\n","  !wget -q -O model.ckpt https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/precompiled/README.md\n","  !mv /content/gdrive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/model.ckpt /content/gdrive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/trashfile.f  \n","  time.sleep(2)\n","  !rm /content/gdrive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/trashfile.f\n","  time.sleep(2)\n","  clear_output()\n","\n","#@markdown Or\n","Path_to_CKPT = \"/content/gdrive/MyDrive/SD_Models\" #@param {type:\"string\"}\n","#@markdown - Insert the full path of your trained model (eg: /content/gdrive/MyDrive/zarathustra.ckpt) or to a folder containing multiple models.\n","\n","#@markdown Or\n","Link_CKPT = \"\" #@param {type:\"string\"}\n","#@markdown - A direct link to a CKPT or a shared gdrive link.\n","\n","def newmdl(token):\n","\n","    if not os.path.exists('/content/gdrive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/model.ckpt'):\n","      if token==\"\":\n","        token=input(\"Insert your huggingface token :\")\n","      %cd /content/\n","      clear_output()\n","      !git init\n","      !git lfs install --system --skip-repo\n","      !$fgitclone --branch fp16 \"https://USER:{token}@huggingface.co/runwayml/stable-diffusion-v1-5\"\n","      if os.path.exists('/content/stable-diffusion-v1-5'):\n","        !$fgitclone \"https://USER:{token}@huggingface.co/stabilityai/sd-vae-ft-mse\"\n","        !rm -r /content/stable-diffusion-v1-5/vae\n","        !mv /content/sd-vae-ft-mse /content/stable-diffusion-v1-5/vae        \n","        !wget -O convertosd.py https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/convertosd.py\n","        !sed -i '201s@.*@    model_path = \"/content/stable-diffusion-v1-5\"@' /content/convertosd.py\n","        !sed -i '202s@.*@    checkpoint_path= \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/model.ckpt\"@' /content/convertosd.py\n","        clear_output()       \n","        !python /content/convertosd.py \n","        if os.path.exists('/content/gdrive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/model.ckpt'):\n","          model='/content/gdrive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/model.ckpt'\n","          clear_output()\n","          print('\u001b[1;32mDONE !')\n","        else:\n","          print('\u001b[1;31mSomething went wrong, try again')\n","      else:\n","        print('\u001b[1;31mMake sure you accept the terms at https://huggingface.co/runwayml/stable-diffusion-v1-5')\n","\n","    else:\n","      model='/content/gdrive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/model.ckpt'\n","      clear_output()\n","      print('\u001b[1;32mModel already exists, check the box \"Redownload_the_original_model\" to redownload/download the V1.5')\n","\n","    if os.path.exists('/content/.git'):\n","      !rm -r /content/.git\n","\n","    if os.path.exists('/content/stable-diffusion-v1-5'):\n","      !rm -r /content/stable-diffusion-v1-5\n","\n","\n","def V2(token):\n","\n","    if not os.path.exists('/content/gdrive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/model.ckpt'):\n","      %cd /content/\n","      clear_output()\n","      !mkdir \"/content/stable-diffusion-V2\"\n","      %cd \"/content/stable-diffusion-V2\"\n","      !git init\n","      !git lfs install --system --skip-repo\n","      if Model_Version == \"V2.1-768px\":\n","        !git remote add -f origin  \"https://USER:{token}@huggingface.co/stabilityai/stable-diffusion-2-1\"\n","      elif Model_Version == \"V2.1-512px\":\n","        !git remote add -f origin  \"https://USER:{token}@huggingface.co/stabilityai/stable-diffusion-2-1-base\" \n","      !git config core.sparsecheckout true\n","      !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nvae\\nfeature_extractor\\nmodel_index.json\" > .git/info/sparse-checkout\n","      !git pull origin fp16\n","      %cd /content\n","      !wget -O convertosdv2.py https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/convertosdv2.py\n","      clear_output()\n","      !python /content/convertosdv2.py --fp16 /content/stable-diffusion-V2 /content/gdrive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/model.ckpt\n","      if os.path.exists('/content/gdrive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/model.ckpt'):\n","        model='/content/gdrive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/model.ckpt'\n","        clear_output()\n","        print('\u001b[1;32mDONE !')\n","      else:\n","        print('\u001b[1;31mSomething went wrong, try again')\n","\n","    else:\n","      model='/content/gdrive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/model.ckpt'\n","      clear_output()\n","      print('\u001b[1;32mModel already exists, check the box \"Redownload_the_original_model\" to redownload/download the V2.')\n","\n","    if os.path.exists('/content/.git'):\n","      !rm -r /content/.git\n","      !rm -r /content/convertosdv2.py\n","    if os.path.exists('/content/stable-diffusion-V2'):\n","      !rm -r /content/stable-diffusion-V2\n","\n","def inpmdl(token):\n","\n","    if not os.path.exists('/content/gdrive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/sd-v1-5-inpainting.ckpt'):\n","      if token==\"\":\n","        token=input(\"Insert your huggingface token :\")\n","      %cd /content/\n","      clear_output()\n","      !git init\n","      !git lfs install --system --skip-repo\n","      !$fgitclone --branch fp16 \"https://USER:{token}@huggingface.co/runwayml/stable-diffusion-inpainting\"\n","      if os.path.exists('/content/stable-diffusion-inpainting'):\n","        !$fgitclone \"https://USER:{token}@huggingface.co/stabilityai/sd-vae-ft-mse\"\n","        !rm -r /content/stable-diffusion-inpainting/vae\n","        !mv /content/sd-vae-ft-mse /content/stable-diffusion-inpainting/vae        \n","        !wget -O convertosd.py https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/convertosd.py\n","        !sed -i '201s@.*@    model_path = \"/content/stable-diffusion-inpainting\"@' /content/convertosd.py\n","        !sed -i '202s@.*@    checkpoint_path= \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/sd-v1-5-inpainting.ckpt\"@' /content/convertosd.py\n","        clear_output()       \n","        !python /content/convertosd.py \n","        if os.path.exists('/content/gdrive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/sd-v1-5-inpainting.ckpt'):\n","          clear_output()\n","          print('\u001b[1;32mDONE !')\n","        else:\n","          print('\u001b[1;31mSomething went wrong, try again')\n","      else:\n","        print('\u001b[1;31mMake sure you have accepted the terms at https://huggingface.co/runwayml/stable-diffusion-inpainting')\n","\n","    else:\n","      clear_output()\n","      print('\u001b[1;32mModel already exists.')\n","\n","    if os.path.exists('/content/.git'):\n","      !rm -r /content/.git\n","\n","    if os.path.exists('/content/stable-diffusion-inpainting'):\n","      !rm -r /content/stable-diffusion-inpainting   \n","\n","\n","if (Path_to_CKPT !=''):\n","  if os.path.exists(str(Path_to_CKPT)):\n","    print('\u001b[1;32mUsing the trained model.')\n","  else:\n","      while not os.path.exists(str(Path_to_CKPT)):\n","        print('\u001b[1;31mWrong path, use the colab file explorer to copy the path : ')\n","        Path_to_CKPT=input()\n","      if os.path.exists(str(Path_to_CKPT)):\n","        print('\u001b[1;32mUsing the trained model.')\n","\n","  model=Path_to_CKPT\n","\n","elif Link_CKPT != \"\":\n","  if os.path.exists('/content/mainmodel.ckpt'):\n","    !rm /content/mainmodel.ckpt\n","  %cd /content/gdrive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion\n","  !wget -q -O model.ckpt https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/precompiled/attention.py\n","  !mv /content/gdrive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/model.ckpt /content/gdrive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/trashfile.f  \n","  time.sleep(2)\n","  !rm /content/gdrive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/trashfile.f\n","  time.sleep(2)\n","  clear_output()\n","  !gdown --fuzzy -O model.ckpt $Link_CKPT\n","  if os.path.exists('/content/gdrive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/model.ckpt'):\n","    if os.path.getsize(\"/content/gdrive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/model.ckpt\") > 1810671599:\n","      model='/content/gdrive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/model.ckpt'\n","      clear_output()\n","      print('\u001b[1;32mModel downloaded, using the trained model.')\n","    else:\n","      print('\u001b[1;31mWrong link, check that the link is valid')\n","  else:\n","    print('\u001b[1;31mWrong link, check that the link is valid')\n","\n","\n","elif Model_Version==\"1.5\":\n","  newmdl(token)\n","\n","elif Model_Version==\"V2.1-512px\" or Model_Version==\"V2.1-768px\":\n","  V2(token)\n","\n","if RunwayML_Inpainting_Model:\n","  inpmdl(token)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PjzwxTkPSPHf","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5dba6c39-674b-4097-a13e-6986c0f270a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["LatentDiffusion: Running in eps-prediction mode\n","DiffusionWrapper has 859.52 M params.\n","Loading weights [925997e9] from /content/gdrive/MyDrive/SD_Models/SD_Models_1/animefinal-full-pruned.ckpt\n","Applying xformers cross attention optimization.\n","Model loaded.\n","/usr/local/lib/python3.8/dist-packages/gradio/deprecation.py:43: UserWarning: You have unused kwarg parameters in Row, please remove them: {'equal_height': True}\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/gradio/utils.py:763: UserWarning: Expected 1 arguments for function <function update_generation_info at 0x7f6afc75b700>, received 2.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/gradio/utils.py:771: UserWarning: Expected maximum 1 arguments for function <function update_generation_info at 0x7f6afc75b700>, received 2.\n","  warnings.warn(\n","Loaded a total of 0 textual inversion embeddings.\n","Embeddings: \n","Running on local URL:  https://angry-tigers-go-34-90-116-227.loca.lt:443\n","\u001b[32mConnected\n","  0% 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n","\n","  5% 1/20 [00:06<02:05,  6.60s/it]\n"," 15% 3/20 [00:06<00:27,  1.59s/it]\n"," 20% 4/20 [00:07<00:16,  1.02s/it]\n"," 25% 5/20 [00:07<00:10,  1.42it/s]\n"," 30% 6/20 [00:07<00:07,  1.94it/s]\n"," 35% 7/20 [00:07<00:05,  2.53it/s]\n"," 40% 8/20 [00:07<00:03,  3.17it/s]\n"," 45% 9/20 [00:07<00:02,  3.80it/s]\n"," 50% 10/20 [00:07<00:02,  4.40it/s]\n"," 55% 11/20 [00:08<00:01,  4.91it/s]\n"," 60% 12/20 [00:08<00:01,  5.36it/s]\n"," 65% 13/20 [00:08<00:01,  5.72it/s]\n"," 70% 14/20 [00:08<00:01,  5.98it/s]\n"," 75% 15/20 [00:08<00:00,  6.19it/s]\n"," 80% 16/20 [00:08<00:00,  6.35it/s]\n"," 85% 17/20 [00:08<00:00,  6.47it/s]\n"," 90% 18/20 [00:09<00:00,  6.53it/s]\n"," 95% 19/20 [00:09<00:00,  6.59it/s]\n","100% 20/20 [00:09<00:00,  2.12it/s]\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.8/dist-packages/gradio/routes.py\", line 284, in run_predict\n","    output = await app.blocks.process_api(\n","  File \"/usr/local/lib/python3.8/dist-packages/gradio/blocks.py\", line 982, in process_api\n","    result = await self.call_function(fn_index, inputs, iterator)\n","  File \"/usr/local/lib/python3.8/dist-packages/gradio/blocks.py\", line 824, in call_function\n","    prediction = await anyio.to_thread.run_sync(\n","  File \"/usr/local/lib/python3.8/dist-packages/anyio/to_thread.py\", line 31, in run_sync\n","    return await get_asynclib().run_sync_in_worker_thread(\n","  File \"/usr/local/lib/python3.8/dist-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n","    return await future\n","  File \"/usr/local/lib/python3.8/dist-packages/anyio/_backends/_asyncio.py\", line 867, in run\n","    result = context.run(func, *args)\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/ui.py\", line 464, in <lambda>\n","    fn=lambda: check_progress_call(id_part),\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/ui.py\", line 210, in check_progress_call\n","    shared.state.set_current_image()\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/shared.py\", line 234, in set_current_image\n","    self.do_set_current_image()\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/shared.py\", line 242, in do_set_current_image\n","    self.assign_current_image(modules.sd_samplers.samples_to_image_grid(self.current_latent))\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_samplers.py\", line 134, in samples_to_image_grid\n","    return images.image_grid([single_sample_to_image(sample, approximation) for sample in samples])\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_samplers.py\", line 134, in <listcomp>\n","    return images.image_grid([single_sample_to_image(sample, approximation) for sample in samples])\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_samplers.py\", line 119, in single_sample_to_image\n","    x_sample = sd_vae_approx.model()(sample.to(devices.device, devices.dtype).unsqueeze(0))[0].detach()\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_vae_approx.py\", line 39, in model\n","    sd_vae_approx_model.load_state_dict(torch.load(os.path.join(paths.models_path, \"VAE-approx\", \"model.pt\")))\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/safe.py\", line 106, in load\n","    return load_with_extra(filename, extra_handler=global_extra_handler, *args, **kwargs)\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/safe.py\", line 151, in load_with_extra\n","    return unsafe_torch_load(filename, *args, **kwargs)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/serialization.py\", line 771, in load\n","    with _open_file_like(f, 'rb') as opened_file:\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/serialization.py\", line 270, in _open_file_like\n","    return _open_file(name_or_buffer, mode)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/serialization.py\", line 251, in __init__\n","    super(_open_file, self).__init__(open(name, mode))\n","FileNotFoundError: [Errno 2] No such file or directory: '/content/gdrive/MyDrive/sd/stable-diffusion-webui/models/VAE-approx/model.pt'\n","Total progress100% 20/20 [00:07<00:00,  2.79it/s]\n","  0% 0/20 [00:00<?, ?it/s]\n","  5% 1/20 [00:00<00:04,  4.72it/s]\n"," 15% 3/20 [00:00<00:02,  6.22it/s]\n"," 20% 4/20 [00:00<00:02,  6.40it/s]\n"," 25% 5/20 [00:00<00:02,  6.56it/s]\n"," 30% 6/20 [00:00<00:02,  6.65it/s]\n"," 35% 7/20 [00:01<00:01,  6.69it/s]\n"," 40% 8/20 [00:01<00:01,  6.71it/s]\n"," 45% 9/20 [00:01<00:01,  6.73it/s]\n"," 50% 10/20 [00:01<00:01,  6.75it/s]\n"," 55% 11/20 [00:01<00:01,  6.76it/s]\n"," 60% 12/20 [00:01<00:01,  6.75it/s]\n"," 65% 13/20 [00:01<00:01,  6.77it/s]\n"," 70% 14/20 [00:02<00:00,  6.79it/s]\n"," 75% 15/20 [00:02<00:00,  6.79it/s]Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.8/dist-packages/gradio/routes.py\", line 284, in run_predict\n","    output = await app.blocks.process_api(\n","  File \"/usr/local/lib/python3.8/dist-packages/gradio/blocks.py\", line 982, in process_api\n","    result = await self.call_function(fn_index, inputs, iterator)\n","  File \"/usr/local/lib/python3.8/dist-packages/gradio/blocks.py\", line 824, in call_function\n","    prediction = await anyio.to_thread.run_sync(\n","  File \"/usr/local/lib/python3.8/dist-packages/anyio/to_thread.py\", line 31, in run_sync\n","    return await get_asynclib().run_sync_in_worker_thread(\n","  File \"/usr/local/lib/python3.8/dist-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n","    return await future\n","  File \"/usr/local/lib/python3.8/dist-packages/anyio/_backends/_asyncio.py\", line 867, in run\n","    result = context.run(func, *args)\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/ui.py\", line 464, in <lambda>\n","    fn=lambda: check_progress_call(id_part),\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/ui.py\", line 210, in check_progress_call\n","    shared.state.set_current_image()\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/shared.py\", line 234, in set_current_image\n","    self.do_set_current_image()\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/shared.py\", line 242, in do_set_current_image\n","    self.assign_current_image(modules.sd_samplers.samples_to_image_grid(self.current_latent))\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_samplers.py\", line 134, in samples_to_image_grid\n","    return images.image_grid([single_sample_to_image(sample, approximation) for sample in samples])\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_samplers.py\", line 134, in <listcomp>\n","    return images.image_grid([single_sample_to_image(sample, approximation) for sample in samples])\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_samplers.py\", line 119, in single_sample_to_image\n","    x_sample = sd_vae_approx.model()(sample.to(devices.device, devices.dtype).unsqueeze(0))[0].detach()\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_vae_approx.py\", line 28, in forward\n","    x = layer(x)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\", line 463, in forward\n","    return self._conv_forward(input, self.weight, self.bias)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\n","    return F.conv2d(input, weight, bias, self.stride,\n","RuntimeError: Input type (c10::Half) and bias type (float) should be the same\n","\n"," 80% 16/20 [00:02<00:00,  6.78it/s]\n"," 85% 17/20 [00:02<00:00,  6.77it/s]\n"," 90% 18/20 [00:02<00:00,  6.78it/s]\n"," 95% 19/20 [00:02<00:00,  6.76it/s]\n","100% 20/20 [00:03<00:00,  6.64it/s]\n","Total progress100% 20/20 [00:03<00:00,  5.84it/s]\n","  0% 0/28 [00:00<?, ?it/s]\n","  4% 1/28 [00:03<01:43,  3.84s/it]\n"," 11% 3/28 [00:04<00:27,  1.11s/it]\n"," 14% 4/28 [00:04<00:19,  1.24it/s]\n"," 18% 5/28 [00:04<00:14,  1.62it/s]\n"," 21% 6/28 [00:04<00:10,  2.01it/s]\n"," 25% 7/28 [00:05<00:08,  2.41it/s]\n"," 29% 8/28 [00:05<00:07,  2.78it/s]\n"," 32% 9/28 [00:05<00:06,  3.10it/s]\n"," 36% 10/28 [00:05<00:05,  3.36it/s]\n"," 39% 11/28 [00:06<00:04,  3.60it/s]\n"," 43% 12/28 [00:06<00:04,  3.76it/s]Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.8/dist-packages/gradio/routes.py\", line 284, in run_predict\n","    output = await app.blocks.process_api(\n","  File \"/usr/local/lib/python3.8/dist-packages/gradio/blocks.py\", line 982, in process_api\n","    result = await self.call_function(fn_index, inputs, iterator)\n","  File \"/usr/local/lib/python3.8/dist-packages/gradio/blocks.py\", line 824, in call_function\n","    prediction = await anyio.to_thread.run_sync(\n","  File \"/usr/local/lib/python3.8/dist-packages/anyio/to_thread.py\", line 31, in run_sync\n","    return await get_asynclib().run_sync_in_worker_thread(\n","  File \"/usr/local/lib/python3.8/dist-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n","    return await future\n","  File \"/usr/local/lib/python3.8/dist-packages/anyio/_backends/_asyncio.py\", line 867, in run\n","    result = context.run(func, *args)\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/ui.py\", line 464, in <lambda>\n","    fn=lambda: check_progress_call(id_part),\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/ui.py\", line 210, in check_progress_call\n","    shared.state.set_current_image()\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/shared.py\", line 234, in set_current_image\n","    self.do_set_current_image()\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/shared.py\", line 242, in do_set_current_image\n","    self.assign_current_image(modules.sd_samplers.samples_to_image_grid(self.current_latent))\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_samplers.py\", line 134, in samples_to_image_grid\n","    return images.image_grid([single_sample_to_image(sample, approximation) for sample in samples])\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_samplers.py\", line 134, in <listcomp>\n","    return images.image_grid([single_sample_to_image(sample, approximation) for sample in samples])\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_samplers.py\", line 119, in single_sample_to_image\n","    x_sample = sd_vae_approx.model()(sample.to(devices.device, devices.dtype).unsqueeze(0))[0].detach()\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_vae_approx.py\", line 28, in forward\n","    x = layer(x)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\", line 463, in forward\n","    return self._conv_forward(input, self.weight, self.bias)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\n","    return F.conv2d(input, weight, bias, self.stride,\n","RuntimeError: Input type (c10::Half) and bias type (float) should be the same\n","\n"," 46% 13/28 [00:06<00:03,  3.85it/s]\n"," 50% 14/28 [00:06<00:03,  4.02it/s]\n"," 54% 15/28 [00:06<00:03,  4.08it/s]\n"," 57% 16/28 [00:07<00:02,  4.13it/s]\n"," 61% 17/28 [00:07<00:02,  4.16it/s]\n"," 64% 18/28 [00:07<00:02,  4.17it/s]\n"," 68% 19/28 [00:07<00:02,  4.19it/s]\n"," 71% 20/28 [00:08<00:01,  4.18it/s]\n"," 75% 21/28 [00:08<00:01,  4.20it/s]\n"," 79% 22/28 [00:08<00:01,  4.21it/s]\n"," 82% 23/28 [00:08<00:01,  4.20it/s]\n"," 86% 24/28 [00:09<00:00,  4.20it/s]\n"," 89% 25/28 [00:09<00:00,  4.18it/s]\n"," 93% 26/28 [00:09<00:00,  4.20it/s]\n"," 96% 27/28 [00:09<00:00,  4.21it/s]\n","100% 28/28 [00:10<00:00,  2.78it/s]\n","Total progress100% 28/28 [00:12<00:00,  2.30it/s]\n","  0% 0/28 [00:00<?, ?it/s]\n","  4% 1/28 [00:00<00:03,  7.53it/s]\n","  7% 2/28 [00:00<00:05,  4.87it/s]\n"," 11% 3/28 [00:00<00:05,  4.60it/s]\n"," 14% 4/28 [00:00<00:05,  4.45it/s]\n"," 18% 5/28 [00:01<00:05,  4.39it/s]\n"," 21% 6/28 [00:01<00:05,  4.32it/s]\n"," 25% 7/28 [00:01<00:04,  4.31it/s]\n"," 29% 8/28 [00:01<00:04,  4.30it/s]\n"," 32% 9/28 [00:02<00:04,  4.29it/s]\n"," 36% 10/28 [00:02<00:04,  4.28it/s]\n"," 39% 11/28 [00:02<00:03,  4.27it/s]\n"," 43% 12/28 [00:02<00:03,  4.27it/s]\n"," 46% 13/28 [00:02<00:03,  4.26it/s]Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.8/dist-packages/gradio/routes.py\", line 284, in run_predict\n","    output = await app.blocks.process_api(\n","  File \"/usr/local/lib/python3.8/dist-packages/gradio/blocks.py\", line 982, in process_api\n","    result = await self.call_function(fn_index, inputs, iterator)\n","  File \"/usr/local/lib/python3.8/dist-packages/gradio/blocks.py\", line 824, in call_function\n","    prediction = await anyio.to_thread.run_sync(\n","  File \"/usr/local/lib/python3.8/dist-packages/anyio/to_thread.py\", line 31, in run_sync\n","    return await get_asynclib().run_sync_in_worker_thread(\n","  File \"/usr/local/lib/python3.8/dist-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n","    return await future\n","  File \"/usr/local/lib/python3.8/dist-packages/anyio/_backends/_asyncio.py\", line 867, in run\n","    result = context.run(func, *args)\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/ui.py\", line 464, in <lambda>\n","    fn=lambda: check_progress_call(id_part),\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/ui.py\", line 210, in check_progress_call\n","    shared.state.set_current_image()\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/shared.py\", line 234, in set_current_image\n","    self.do_set_current_image()\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/shared.py\", line 242, in do_set_current_image\n","    self.assign_current_image(modules.sd_samplers.samples_to_image_grid(self.current_latent))\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_samplers.py\", line 134, in samples_to_image_grid\n","    return images.image_grid([single_sample_to_image(sample, approximation) for sample in samples])\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_samplers.py\", line 134, in <listcomp>\n","    return images.image_grid([single_sample_to_image(sample, approximation) for sample in samples])\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_samplers.py\", line 119, in single_sample_to_image\n","    x_sample = sd_vae_approx.model()(sample.to(devices.device, devices.dtype).unsqueeze(0))[0].detach()\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_vae_approx.py\", line 28, in forward\n","    x = layer(x)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\", line 463, in forward\n","    return self._conv_forward(input, self.weight, self.bias)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\n","    return F.conv2d(input, weight, bias, self.stride,\n","RuntimeError: Input type (c10::Half) and bias type (float) should be the same\n","\n"," 50% 14/28 [00:03<00:03,  4.25it/s]\n"," 54% 15/28 [00:03<00:03,  4.25it/s]\n"," 57% 16/28 [00:03<00:02,  4.23it/s]\n"," 61% 17/28 [00:03<00:02,  4.24it/s]\n"," 64% 18/28 [00:04<00:02,  4.23it/s]\n"," 68% 19/28 [00:04<00:02,  4.22it/s]\n"," 71% 20/28 [00:04<00:01,  4.22it/s]\n"," 75% 21/28 [00:04<00:01,  4.23it/s]\n"," 79% 22/28 [00:05<00:01,  4.22it/s]\n"," 82% 23/28 [00:05<00:01,  4.23it/s]\n"," 86% 24/28 [00:05<00:00,  4.24it/s]\n"," 89% 25/28 [00:05<00:00,  4.25it/s]\n"," 93% 26/28 [00:06<00:00,  4.24it/s]\n"," 96% 27/28 [00:06<00:00,  4.24it/s]\n","100% 28/28 [00:06<00:00,  4.30it/s]\n","Total progress100% 28/28 [00:07<00:00,  3.77it/s]\n","Loading weights [635d7f6f] from /content/gdrive/MyDrive/SD_Models/SD_Models_2/Nashe (Any+SD+F222-SD_005_1).ckpt\n","Applying xformers cross attention optimization.\n","Weights loaded.\n","  0% 0/28 [00:00<?, ?it/s]\n","  4% 1/28 [00:00<00:04,  6.62it/s]\n","  7% 2/28 [00:00<00:05,  4.57it/s]\n"," 11% 3/28 [00:00<00:05,  4.47it/s]\n"," 14% 4/28 [00:00<00:05,  4.38it/s]\n"," 18% 5/28 [00:01<00:05,  4.35it/s]\n"," 21% 6/28 [00:01<00:05,  4.31it/s]\n"," 25% 7/28 [00:01<00:04,  4.32it/s]\n"," 29% 8/28 [00:01<00:04,  4.30it/s]\n"," 32% 9/28 [00:02<00:04,  4.31it/s]\n"," 36% 10/28 [00:02<00:04,  4.28it/s]\n"," 39% 11/28 [00:02<00:03,  4.30it/s]\n"," 43% 12/28 [00:02<00:03,  4.26it/s]\n"," 46% 13/28 [00:02<00:03,  4.26it/s]Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.8/dist-packages/gradio/routes.py\", line 284, in run_predict\n","    output = await app.blocks.process_api(\n","  File \"/usr/local/lib/python3.8/dist-packages/gradio/blocks.py\", line 982, in process_api\n","    result = await self.call_function(fn_index, inputs, iterator)\n","  File \"/usr/local/lib/python3.8/dist-packages/gradio/blocks.py\", line 824, in call_function\n","    prediction = await anyio.to_thread.run_sync(\n","  File \"/usr/local/lib/python3.8/dist-packages/anyio/to_thread.py\", line 31, in run_sync\n","    return await get_asynclib().run_sync_in_worker_thread(\n","  File \"/usr/local/lib/python3.8/dist-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n","    return await future\n","  File \"/usr/local/lib/python3.8/dist-packages/anyio/_backends/_asyncio.py\", line 867, in run\n","    result = context.run(func, *args)\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/ui.py\", line 464, in <lambda>\n","    fn=lambda: check_progress_call(id_part),\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/ui.py\", line 210, in check_progress_call\n","    shared.state.set_current_image()\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/shared.py\", line 234, in set_current_image\n","    self.do_set_current_image()\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/shared.py\", line 242, in do_set_current_image\n","    self.assign_current_image(modules.sd_samplers.samples_to_image_grid(self.current_latent))\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_samplers.py\", line 134, in samples_to_image_grid\n","    return images.image_grid([single_sample_to_image(sample, approximation) for sample in samples])\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_samplers.py\", line 134, in <listcomp>\n","    return images.image_grid([single_sample_to_image(sample, approximation) for sample in samples])\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_samplers.py\", line 119, in single_sample_to_image\n","    x_sample = sd_vae_approx.model()(sample.to(devices.device, devices.dtype).unsqueeze(0))[0].detach()\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_vae_approx.py\", line 28, in forward\n","    x = layer(x)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\", line 463, in forward\n","    return self._conv_forward(input, self.weight, self.bias)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\n","    return F.conv2d(input, weight, bias, self.stride,\n","RuntimeError: Input type (c10::Half) and bias type (float) should be the same\n","\n"," 50% 14/28 [00:03<00:03,  4.27it/s]\n"," 54% 15/28 [00:03<00:03,  4.27it/s]\n"," 57% 16/28 [00:03<00:02,  4.25it/s]\n"," 61% 17/28 [00:03<00:02,  4.26it/s]\n"," 64% 18/28 [00:04<00:02,  4.27it/s]\n"," 68% 19/28 [00:04<00:02,  4.25it/s]\n"," 71% 20/28 [00:04<00:01,  4.26it/s]\n"," 75% 21/28 [00:04<00:01,  4.26it/s]\n"," 79% 22/28 [00:05<00:01,  4.26it/s]\n"," 82% 23/28 [00:05<00:01,  4.27it/s]\n"," 86% 24/28 [00:05<00:00,  4.25it/s]\n"," 89% 25/28 [00:05<00:00,  4.26it/s]\n"," 93% 26/28 [00:06<00:00,  4.24it/s]\n"," 96% 27/28 [00:06<00:00,  4.23it/s]\n","100% 28/28 [00:06<00:00,  4.30it/s]\n","Total progress100% 28/28 [00:07<00:00,  3.76it/s]\n","  0% 0/28 [00:00<?, ?it/s]\n","  4% 1/28 [00:00<00:03,  7.37it/s]\n","  7% 2/28 [00:00<00:05,  4.91it/s]\n"," 11% 3/28 [00:00<00:05,  4.52it/s]\n"," 14% 4/28 [00:00<00:05,  4.47it/s]\n"," 18% 5/28 [00:01<00:05,  4.37it/s]\n"," 21% 6/28 [00:01<00:05,  4.33it/s]\n"," 25% 7/28 [00:01<00:04,  4.31it/s]\n"," 29% 8/28 [00:01<00:04,  4.19it/s]\n"," 32% 9/28 [00:02<00:04,  4.33it/s]\n"," 36% 10/28 [00:02<00:04,  4.24it/s]\n"," 39% 11/28 [00:02<00:04,  4.25it/s]\n"," 43% 12/28 [00:02<00:03,  4.33it/s]\n"," 46% 13/28 [00:02<00:03,  4.29it/s]Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.8/dist-packages/gradio/routes.py\", line 284, in run_predict\n","    output = await app.blocks.process_api(\n","  File \"/usr/local/lib/python3.8/dist-packages/gradio/blocks.py\", line 982, in process_api\n","    result = await self.call_function(fn_index, inputs, iterator)\n","  File \"/usr/local/lib/python3.8/dist-packages/gradio/blocks.py\", line 824, in call_function\n","    prediction = await anyio.to_thread.run_sync(\n","  File \"/usr/local/lib/python3.8/dist-packages/anyio/to_thread.py\", line 31, in run_sync\n","    return await get_asynclib().run_sync_in_worker_thread(\n","  File \"/usr/local/lib/python3.8/dist-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n","    return await future\n","  File \"/usr/local/lib/python3.8/dist-packages/anyio/_backends/_asyncio.py\", line 867, in run\n","    result = context.run(func, *args)\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/ui.py\", line 464, in <lambda>\n","    fn=lambda: check_progress_call(id_part),\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/ui.py\", line 210, in check_progress_call\n","    shared.state.set_current_image()\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/shared.py\", line 234, in set_current_image\n","    self.do_set_current_image()\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/shared.py\", line 242, in do_set_current_image\n","    self.assign_current_image(modules.sd_samplers.samples_to_image_grid(self.current_latent))\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_samplers.py\", line 134, in samples_to_image_grid\n","    return images.image_grid([single_sample_to_image(sample, approximation) for sample in samples])\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_samplers.py\", line 134, in <listcomp>\n","    return images.image_grid([single_sample_to_image(sample, approximation) for sample in samples])\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_samplers.py\", line 119, in single_sample_to_image\n","    x_sample = sd_vae_approx.model()(sample.to(devices.device, devices.dtype).unsqueeze(0))[0].detach()\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_vae_approx.py\", line 28, in forward\n","    x = layer(x)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\", line 463, in forward\n","    return self._conv_forward(input, self.weight, self.bias)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\n","    return F.conv2d(input, weight, bias, self.stride,\n","RuntimeError: Input type (c10::Half) and bias type (float) should be the same\n","\n"," 50% 14/28 [00:03<00:03,  4.25it/s]\n"," 54% 15/28 [00:03<00:03,  4.26it/s]\n"," 57% 16/28 [00:03<00:02,  4.27it/s]\n"," 61% 17/28 [00:03<00:02,  4.25it/s]\n"," 64% 18/28 [00:04<00:02,  4.22it/s]\n"," 68% 19/28 [00:04<00:02,  4.21it/s]\n"," 71% 20/28 [00:04<00:01,  4.19it/s]\n"," 75% 21/28 [00:04<00:01,  4.25it/s]\n"," 79% 22/28 [00:05<00:01,  4.17it/s]\n"," 82% 23/28 [00:05<00:01,  4.21it/s]\n"," 86% 24/28 [00:05<00:00,  4.22it/s]\n"," 89% 25/28 [00:05<00:00,  4.15it/s]\n"," 93% 26/28 [00:06<00:00,  4.20it/s]\n"," 96% 27/28 [00:06<00:00,  4.18it/s]\n","100% 28/28 [00:06<00:00,  4.28it/s]\n","Total progress100% 28/28 [00:07<00:00,  3.72it/s]\n","  0% 0/28 [00:00<?, ?it/s]\n","  4% 1/28 [00:00<00:03,  7.52it/s]\n","  7% 2/28 [00:00<00:05,  4.55it/s]\n"," 11% 3/28 [00:00<00:05,  4.44it/s]\n"," 14% 4/28 [00:00<00:05,  4.36it/s]\n"," 18% 5/28 [00:01<00:05,  4.34it/s]\n"," 21% 6/28 [00:01<00:05,  4.29it/s]\n"," 25% 7/28 [00:01<00:04,  4.30it/s]\n"," 29% 8/28 [00:01<00:04,  4.29it/s]\n"," 32% 9/28 [00:02<00:04,  4.26it/s]\n"," 36% 10/28 [00:02<00:04,  4.24it/s]\n"," 39% 11/28 [00:02<00:04,  4.23it/s]\n"," 43% 12/28 [00:02<00:03,  4.22it/s]\n"," 46% 13/28 [00:03<00:03,  4.23it/s]\n"," 50% 14/28 [00:03<00:03,  4.23it/s]\n"," 54% 15/28 [00:03<00:03,  4.23it/s]Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.8/dist-packages/gradio/routes.py\", line 284, in run_predict\n","    output = await app.blocks.process_api(\n","  File \"/usr/local/lib/python3.8/dist-packages/gradio/blocks.py\", line 982, in process_api\n","    result = await self.call_function(fn_index, inputs, iterator)\n","  File \"/usr/local/lib/python3.8/dist-packages/gradio/blocks.py\", line 824, in call_function\n","    prediction = await anyio.to_thread.run_sync(\n","  File \"/usr/local/lib/python3.8/dist-packages/anyio/to_thread.py\", line 31, in run_sync\n","    return await get_asynclib().run_sync_in_worker_thread(\n","  File \"/usr/local/lib/python3.8/dist-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n","    return await future\n","  File \"/usr/local/lib/python3.8/dist-packages/anyio/_backends/_asyncio.py\", line 867, in run\n","    result = context.run(func, *args)\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/ui.py\", line 464, in <lambda>\n","    fn=lambda: check_progress_call(id_part),\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/ui.py\", line 210, in check_progress_call\n","    shared.state.set_current_image()\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/shared.py\", line 234, in set_current_image\n","    self.do_set_current_image()\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/shared.py\", line 242, in do_set_current_image\n","    self.assign_current_image(modules.sd_samplers.samples_to_image_grid(self.current_latent))\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_samplers.py\", line 134, in samples_to_image_grid\n","    return images.image_grid([single_sample_to_image(sample, approximation) for sample in samples])\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_samplers.py\", line 134, in <listcomp>\n","    return images.image_grid([single_sample_to_image(sample, approximation) for sample in samples])\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_samplers.py\", line 119, in single_sample_to_image\n","    x_sample = sd_vae_approx.model()(sample.to(devices.device, devices.dtype).unsqueeze(0))[0].detach()\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_vae_approx.py\", line 28, in forward\n","    x = layer(x)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\", line 463, in forward\n","    return self._conv_forward(input, self.weight, self.bias)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\n","    return F.conv2d(input, weight, bias, self.stride,\n","RuntimeError: Input type (c10::Half) and bias type (float) should be the same\n","\n"," 57% 16/28 [00:03<00:02,  4.22it/s]\n"," 61% 17/28 [00:03<00:02,  4.23it/s]\n"," 64% 18/28 [00:04<00:02,  4.21it/s]\n"," 68% 19/28 [00:04<00:02,  4.22it/s]\n"," 71% 20/28 [00:04<00:01,  4.23it/s]\n"," 75% 21/28 [00:04<00:01,  4.22it/s]\n"," 79% 22/28 [00:05<00:01,  4.22it/s]\n"," 82% 23/28 [00:05<00:01,  4.23it/s]\n"," 86% 24/28 [00:05<00:00,  4.20it/s]\n"," 89% 25/28 [00:05<00:00,  4.20it/s]\n"," 93% 26/28 [00:06<00:00,  4.20it/s]\n"," 96% 27/28 [00:06<00:00,  4.22it/s]\n","100% 28/28 [00:06<00:00,  4.27it/s]\n","Total progress100% 28/28 [00:07<00:00,  3.72it/s]\n","  0% 0/28 [00:00<?, ?it/s]\n","  4% 1/28 [00:00<00:03,  7.93it/s]\n","  7% 2/28 [00:00<00:05,  4.72it/s]\n"," 11% 3/28 [00:00<00:05,  4.51it/s]\n"," 14% 4/28 [00:00<00:05,  4.39it/s]\n"," 18% 5/28 [00:01<00:05,  4.34it/s]\n"," 21% 6/28 [00:01<00:05,  4.28it/s]\n"," 25% 7/28 [00:01<00:04,  4.30it/s]\n"," 29% 8/28 [00:01<00:04,  4.29it/s]\n"," 32% 9/28 [00:02<00:04,  4.27it/s]\n"," 36% 10/28 [00:02<00:04,  4.24it/s]\n"," 39% 11/28 [00:02<00:04,  4.20it/s]\n"," 43% 12/28 [00:02<00:03,  4.25it/s]\n"," 46% 13/28 [00:03<00:03,  4.19it/s]\n"," 50% 14/28 [00:03<00:03,  4.27it/s]Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.8/dist-packages/gradio/routes.py\", line 284, in run_predict\n","    output = await app.blocks.process_api(\n","  File \"/usr/local/lib/python3.8/dist-packages/gradio/blocks.py\", line 982, in process_api\n","    result = await self.call_function(fn_index, inputs, iterator)\n","  File \"/usr/local/lib/python3.8/dist-packages/gradio/blocks.py\", line 824, in call_function\n","    prediction = await anyio.to_thread.run_sync(\n","  File \"/usr/local/lib/python3.8/dist-packages/anyio/to_thread.py\", line 31, in run_sync\n","    return await get_asynclib().run_sync_in_worker_thread(\n","  File \"/usr/local/lib/python3.8/dist-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n","    return await future\n","  File \"/usr/local/lib/python3.8/dist-packages/anyio/_backends/_asyncio.py\", line 867, in run\n","    result = context.run(func, *args)\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/ui.py\", line 464, in <lambda>\n","    fn=lambda: check_progress_call(id_part),\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/ui.py\", line 210, in check_progress_call\n","    shared.state.set_current_image()\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/shared.py\", line 234, in set_current_image\n","    self.do_set_current_image()\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/shared.py\", line 242, in do_set_current_image\n","    self.assign_current_image(modules.sd_samplers.samples_to_image_grid(self.current_latent))\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_samplers.py\", line 134, in samples_to_image_grid\n","    return images.image_grid([single_sample_to_image(sample, approximation) for sample in samples])\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_samplers.py\", line 134, in <listcomp>\n","    return images.image_grid([single_sample_to_image(sample, approximation) for sample in samples])\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_samplers.py\", line 119, in single_sample_to_image\n","    x_sample = sd_vae_approx.model()(sample.to(devices.device, devices.dtype).unsqueeze(0))[0].detach()\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_vae_approx.py\", line 28, in forward\n","    x = layer(x)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\", line 463, in forward\n","    return self._conv_forward(input, self.weight, self.bias)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\n","    return F.conv2d(input, weight, bias, self.stride,\n","RuntimeError: Input type (c10::Half) and bias type (float) should be the same\n","\n"," 54% 15/28 [00:03<00:03,  4.24it/s]\n"," 57% 16/28 [00:03<00:02,  4.25it/s]\n"," 61% 17/28 [00:03<00:02,  4.23it/s]\n"," 64% 18/28 [00:04<00:02,  4.22it/s]\n"," 68% 19/28 [00:04<00:02,  4.22it/s]\n"," 71% 20/28 [00:04<00:01,  4.22it/s]\n"," 75% 21/28 [00:04<00:01,  4.21it/s]\n"," 79% 22/28 [00:05<00:01,  4.15it/s]\n"," 82% 23/28 [00:05<00:01,  4.21it/s]\n"," 86% 24/28 [00:05<00:00,  4.22it/s]\n"," 89% 25/28 [00:05<00:00,  4.21it/s]\n"," 93% 26/28 [00:06<00:00,  4.18it/s]\n"," 96% 27/28 [00:06<00:00,  4.20it/s]\n","100% 28/28 [00:06<00:00,  4.27it/s]\n","Total progress100% 28/28 [00:07<00:00,  3.73it/s]\n","  0% 0/28 [00:00<?, ?it/s]\n","  4% 1/28 [00:00<00:03,  7.54it/s]\n","  7% 2/28 [00:00<00:05,  5.07it/s]\n"," 11% 3/28 [00:00<00:05,  4.65it/s]\n"," 14% 4/28 [00:00<00:05,  4.49it/s]\n"," 18% 5/28 [00:01<00:05,  4.38it/s]\n"," 21% 6/28 [00:01<00:05,  4.36it/s]\n"," 25% 7/28 [00:01<00:04,  4.31it/s]\n"," 29% 8/28 [00:01<00:04,  4.25it/s]\n"," 32% 9/28 [00:02<00:04,  4.25it/s]\n"," 36% 10/28 [00:02<00:04,  4.24it/s]\n"," 39% 11/28 [00:02<00:04,  4.23it/s]\n"," 43% 12/28 [00:02<00:03,  4.22it/s]\n"," 46% 13/28 [00:02<00:03,  4.23it/s]Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.8/dist-packages/gradio/routes.py\", line 284, in run_predict\n","    output = await app.blocks.process_api(\n","  File \"/usr/local/lib/python3.8/dist-packages/gradio/blocks.py\", line 982, in process_api\n","    result = await self.call_function(fn_index, inputs, iterator)\n","  File \"/usr/local/lib/python3.8/dist-packages/gradio/blocks.py\", line 824, in call_function\n","    prediction = await anyio.to_thread.run_sync(\n","  File \"/usr/local/lib/python3.8/dist-packages/anyio/to_thread.py\", line 31, in run_sync\n","    return await get_asynclib().run_sync_in_worker_thread(\n","  File \"/usr/local/lib/python3.8/dist-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n","    return await future\n","  File \"/usr/local/lib/python3.8/dist-packages/anyio/_backends/_asyncio.py\", line 867, in run\n","    result = context.run(func, *args)\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/ui.py\", line 464, in <lambda>\n","    fn=lambda: check_progress_call(id_part),\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/ui.py\", line 210, in check_progress_call\n","    shared.state.set_current_image()\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/shared.py\", line 234, in set_current_image\n","    self.do_set_current_image()\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/shared.py\", line 242, in do_set_current_image\n","    self.assign_current_image(modules.sd_samplers.samples_to_image_grid(self.current_latent))\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_samplers.py\", line 134, in samples_to_image_grid\n","    return images.image_grid([single_sample_to_image(sample, approximation) for sample in samples])\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_samplers.py\", line 134, in <listcomp>\n","    return images.image_grid([single_sample_to_image(sample, approximation) for sample in samples])\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_samplers.py\", line 119, in single_sample_to_image\n","    x_sample = sd_vae_approx.model()(sample.to(devices.device, devices.dtype).unsqueeze(0))[0].detach()\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_vae_approx.py\", line 28, in forward\n","    x = layer(x)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\", line 463, in forward\n","    return self._conv_forward(input, self.weight, self.bias)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\n","    return F.conv2d(input, weight, bias, self.stride,\n","RuntimeError: Input type (c10::Half) and bias type (float) should be the same\n","\n"," 50% 14/28 [00:03<00:03,  4.22it/s]\n"," 54% 15/28 [00:03<00:03,  4.21it/s]\n"," 57% 16/28 [00:03<00:02,  4.23it/s]\n"," 61% 17/28 [00:03<00:02,  4.23it/s]\n"," 64% 18/28 [00:04<00:02,  4.23it/s]\n"," 68% 19/28 [00:04<00:02,  4.23it/s]\n"," 71% 20/28 [00:04<00:01,  4.22it/s]\n"," 75% 21/28 [00:04<00:01,  4.21it/s]\n"," 79% 22/28 [00:05<00:01,  4.21it/s]\n"," 82% 23/28 [00:05<00:01,  4.21it/s]\n"," 86% 24/28 [00:05<00:00,  4.18it/s]\n"," 89% 25/28 [00:05<00:00,  4.18it/s]\n"," 93% 26/28 [00:06<00:00,  4.19it/s]\n"," 96% 27/28 [00:06<00:00,  4.20it/s]\n","100% 28/28 [00:06<00:00,  4.28it/s]\n","Total progress100% 28/28 [00:07<00:00,  3.73it/s]\n"]}],"source":["#@markdown # Start stable-diffusion\n","from IPython.utils import capture\n","import time\n","import sys\n","import fileinput\n","\n","Model_Version = \"1.5\" #@param [\"1.5\", \"V2.1-512\", \"V2.1-768\"]\n","#@markdown  - Important! Choose the correct version and resolution of the model\n","\n","Use_Gradio_Server = False #@param {type:\"boolean\"}\n","#@markdown  - Only if you have trouble connecting to the local server\n","\n","Enable_API = False #@param {type:\"boolean\"}\n","\n","Large_Model= False #@param {type:\"boolean\"}\n","#@markdown  - Check if you have trouble loading a model 7GB+\n","\n","Dark_Theme = True #@param {type:\"boolean\"}\n","\n","if Large_Model:\n","  !sed -i 's@cmd_opts.lowram else \\\"cpu\\\"@cmd_opts.lowram else \\\"cuda\\\"@' /content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/shared.py\n","else:\n","  !sed -i 's@cmd_opts.lowram else \\\"cuda\\\"@cmd_opts.lowram else \\\"cpu\\\"@' /content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/shared.py\n","   \n","with capture.capture_output() as cap: \n","  if not os.path.exists('/tools/node/bin/lt'):\n","    !npm install -g localtunnel\n","\n","with capture.capture_output() as cap: \n","  %cd /content/gdrive/MyDrive/sd/stable-diffusion-webui/\n","  time.sleep(1)\n","  !wget -O webui.py https://raw.githubusercontent.com/DHawk34/FSD-25.12.22/master/stable-diffusion-webui/webui.py\n","  !sed -i 's@ui.create_ui().*@ui.create_ui();shared.demo.queue(concurrency_count=999999,status_update_rate=0.1)@' /content/gdrive/MyDrive/sd/stable-diffusion-webui/webui.py\n","  %cd /content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/\n","  !wget -O ui.py https://raw.githubusercontent.com/DHawk34/FSD-25.12.22/master/stable-diffusion-webui/modules/ui.py\n","  !sed -i 's@css = \"\".*@with open(os.path.join(script_path, \"style.css\"), \"r\", encoding=\"utf8\") as file:\\n        css = file.read()@' /content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/ui.py\n","  %cd /content/gdrive/MyDrive/sd/stable-diffusion-webui\n","  !wget -O style.css https://raw.githubusercontent.com/DHawk34/FSD-25.12.22/master/stable-diffusion-webui/style.css\n","  !sed -i 's@min-height: 4.*@min-height: 5.5em;@g' /content/gdrive/MyDrive/sd/stable-diffusion-webui/style.css\n","  !sed -i \"s@'extensions/depthmap2mask/scripts/depthmap_for_depth2img.py'@\\\"/content/gdrive/MyDrive/sd/stable-diffusion-webui/extensions/depthmap2mask/scripts/depthmap_for_depth2img.py\\\"@\" /content/gdrive/MyDrive/sd/stable-diffusion-webui/extensions/depthmap2mask/scripts/depth2image_depthmask.py\n","  !sed -i 's@\"repositories/BoostingMonocularDepth\"@\\\"/content/gdrive/MyDrive/sd/stablediffusion/repositories/BoostingMonocularDepth\\\"@' /content/gdrive/MyDrive/sd/stable-diffusion-webui/extensions/stable-diffusion-webui-depthmap-script/scripts/depthmap.py\n","  !sed -i 's@\"multiple_tqdm\": true,@\\\"multiple_tqdm\": false,@' /content/gdrive/MyDrive/sd/stable-diffusion-webui/config.json\n","  !sed -i '902s@.*@        self.logvar = self.logvar.to(self.device)@' /content/gdrive/MyDrive/sd/stablediffusion/ldm/models/diffusion/ddpm.py  \n","  %cd /content\n","\n","\n","share=''\n","if Use_Gradio_Server:\n","  share='--share'\n","  for line in fileinput.input('/usr/local/lib/python3.8/dist-packages/gradio/blocks.py', inplace=True):\n","    if line.strip().startswith('self.server_name ='):\n","        line = '            self.server_name = server_name\\n'\n","    if line.strip().startswith('self.server_port ='):\n","        line = '            self.server_port = server_port\\n'\n","    sys.stdout.write(line)\n","  clear_output()\n","  \n","else:\n","  share=''\n","  !nohup lt --port 7860 > srv.txt 2>&1 &\n","  time.sleep(2)\n","  !grep -o 'https[^ ]*' /content/srv.txt >srvr.txt\n","  time.sleep(2)\n","  srv= getoutput('cat /content/srvr.txt')\n","\n","  for line in fileinput.input('/usr/local/lib/python3.8/dist-packages/gradio/blocks.py', inplace=True):\n","    if line.strip().startswith('self.server_name ='):\n","        line = f'            self.server_name = \"{srv[8:]}\"\\n'\n","    if line.strip().startswith('self.server_port ='):\n","        line = '            self.server_port = 443\\n'\n","    if line.strip().startswith('self.protocol = \"https\"'):\n","        line = '            self.protocol = \"https\"\\n'\n","    if line.strip().startswith('if self.local_url.startswith(\"https\") or self.is_colab'):\n","        line = ''    \n","    if line.strip().startswith('else \"http\"'):\n","        line = ''              \n","    sys.stdout.write(line)\n","          \n","  !sed -i '13s@.*@    \"PUBLIC_SHARE_TRUE\": \"\u001b[32mConnected\",@' /usr/local/lib/python3.8/dist-packages/gradio/strings.py\n","  \n","  !rm /content/srv.txt\n","  !rm /content/srvr.txt\n","\n","with capture.capture_output() as cap: \n","  %cd /content/gdrive/MyDrive/sd/stablediffusion\n","\n","api = '--api' if Enable_API else ''\n","theme='--theme=dark' if Dark_Theme else ''\n","\n","if Model_Version == \"V2.1-768\":\n","  configf=\"--config /content/gdrive/MyDrive/sd/stablediffusion/configs/stable-diffusion/v2-inference-v.yaml\"\n","  !sed -i 's@def load_state_dict(checkpoint_path: str, map_location.*@def load_state_dict(checkpoint_path: str, map_location=\"cuda\"):@' /usr/local/lib/python3.8/dist-packages/open_clip/factory.py  \n","elif Model_Version == \"V2.1-512\":\n","  configf=\"--config /content/gdrive/MyDrive/sd/stablediffusion/configs/stable-diffusion/v2-inference.yaml\"\n","  !sed -i 's@def load_state_dict(checkpoint_path: str, map_location.*@def load_state_dict(checkpoint_path: str, map_location=\"cuda\"):@' /usr/local/lib/python3.8/dist-packages/open_clip/factory.py  \n","else:\n","  configf=\"\"\n","  !sed -i 's@def load_state_dict(checkpoint_path: str, map_location.*@def load_state_dict(checkpoint_path: str, map_location=\"cpu\"):@' /usr/local/lib/python3.8/dist-packages/open_clip/factory.py  \n","\n","if os.path.exists('/usr/local/lib/python3.8/dist-packages/xformers'):\n","  xformers=\"--xformers\" \n","else:\n","  xformers=\"\"\n","\n","try:\n","  model\n","  if os.path.isfile(model):\n","    !python /content/gdrive/MyDrive/sd/stable-diffusion-webui/webui.py $share $api $theme --disable-safe-unpickle --enable-insecure-extension-access --no-half-vae  --ckpt \"$model\" $configf $xformers\n","  else:\n","    !python /content/gdrive/MyDrive/sd/stable-diffusion-webui/webui.py $share $api $theme --disable-safe-unpickle --enable-insecure-extension-access --no-half-vae  --ckpt-dir \"$model\" $configf $xformers\n","except:\n","   !python /content/gdrive/MyDrive/sd/stable-diffusion-webui/webui.py $share $api $theme --disable-safe-unpickle --enable-insecure-extension-access --no-half-vae $configf $xformers"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}